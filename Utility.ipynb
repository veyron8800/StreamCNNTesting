{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import shutil\n",
    "import hls4ml\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Creation\n",
    "test_data_dir = 'TestData'\n",
    "os.makedirs(test_data_dir, exist_ok=True)\n",
    "\n",
    "syn_dir = 'Synthesis'\n",
    "os.makedirs(syn_dir, exist_ok=True)\n",
    "\n",
    "model_files_dir = 'ModelFiles'\n",
    "os.makedirs(model_files_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve Function\n",
    "def roc(model, test_in, test_truth):\n",
    "    try:\n",
    "        model_iter = iter(model)\n",
    "    except TypeError:\n",
    "        model_iter = iter([model])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    plt.grid(which='both')\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    \n",
    "    for m in model_iter:\n",
    "        p = m.predict(test_in)[:,0]\n",
    "        fpr, tpr, threshold = metrics.roc_curve(test_truth, p, drop_intermediate=False)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        x = np.linspace(0,1,1000)\n",
    "        y = np.interp(x=x, xp=fpr, fp=tpr)\n",
    "        plt.plot(x, y, linewidth=3, label=f'{m.name} (AUC={auc:.4f})')\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1],color='r', linestyle='--', linewidth=3, label='Coin Flip')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training wrapper\n",
    "# Trains multiple models and returns the one with the best auc metric\n",
    "def train_best_auc(model, train_in, train_truth, test_in, test_truth, training_passes=10, epochs=10, batch_size=32, callbacks=[], skip_reload=False):\n",
    "    path = f'{model_files_dir}/{model.name}.h5'\n",
    "    if os.path.exists(path) and not skip_reload:\n",
    "        return keras.models.load_model(path)\n",
    "    else:\n",
    "        best_auc = 0\n",
    "        best_model = None\n",
    "        for i in range(training_passes):\n",
    "            m = keras.models.clone_model(model)\n",
    "            m.compile(optimizer='Nadam', loss=keras.losses.binary_crossentropy, metrics=[keras.metrics.BinaryAccuracy(), keras.metrics.AUC()])\n",
    "            m.fit(train_in, train_truth, epochs=epochs, batch_size=batch_size, validation_data=(test_in, test_truth), callbacks=callbacks)\n",
    "            p = m.predict(test_in)[:,0]\n",
    "            fpr, tpr, threshold = metrics.roc_curve(test_truth, p)\n",
    "            auc = metrics.auc(fpr, tpr)\n",
    "            print(f'auc = {auc}' + '\\n' + ('-'*50))\n",
    "            if auc > best_auc:\n",
    "                best_auc = auc\n",
    "                best_model = m\n",
    "        \n",
    "        best_model.save(path)\n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, inference_results_path):\n",
    "    with open(inference_results_path, 'r') as fin:\n",
    "        y_pred = np.array(fin.read().split('\\n')[:len(y_true)]).astype(float)\n",
    "    \n",
    "    binary_accuracy = len(np.where(((y_true==1)&(y_pred>=0.5))|((y_true==0)&(y_pred<0.5)))[0])/len(y_true)\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred, drop_intermediate=False)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    return binary_accuracy, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sim_rpt(report_path):\n",
    "    with open(report_path, 'r') as fin:\n",
    "        sim_rpt = {}\n",
    "        for line in fin:\n",
    "            key, value = line.split('=')\n",
    "            key = key.replace('$', '').replace(' ', '')\n",
    "            value = int(value.replace('\\n', '').replace(' ', '').replace('\"', ''))\n",
    "            sim_rpt[key] = value\n",
    "    \n",
    "    return sim_rpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesize HLS Model\n",
    "def synthesize_model(model, io_type, ap_type, strategy, csim=False, cosim=False,\n",
    "                     validate_sim_results=False, show_quantization_plots=False,\n",
    "                     test_input=None, test_truth=None, delete_files=False):\n",
    "    # Input validation\n",
    "    if (validate_sim_results and test_input is None and test_truth is None) or \\\n",
    "        (show_quantization_plots and test_input is None):\n",
    "        raise Exception('Invalid parameter combination. Test data is required.')\n",
    "        \n",
    "    # Define all return values as null\n",
    "    csim_binary_accuracy = None\n",
    "    csim_auc = None\n",
    "    cosim_binary_accuracy = None\n",
    "    cosim_auc = None\n",
    "    syn_xml = None\n",
    "    sim_rpt = None\n",
    "    \n",
    "    # Try-catch block used to always move the cwd back to the original\n",
    "    try:\n",
    "        with open('config_template.yml', 'r') as fin:\n",
    "            cfg_template = fin.read()\n",
    "\n",
    "        # Change cwd into synthesis dir\n",
    "        path = f'{syn_dir}/{model.name}'\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        \n",
    "        os.makedirs(path)\n",
    "        os.chdir(path)\n",
    "\n",
    "        # Write model.h5 file\n",
    "        model.save(f'{model.name}.h5')\n",
    "\n",
    "        # Write test data to .dat\n",
    "        if validate_sim_results:\n",
    "            test_data_prefix = ''\n",
    "            os.makedirs(test_data_dir, exist_ok=True)\n",
    "            with open(test_data_dir + '/test_images.dat', 'w') as input_out, open(test_data_dir + '/test_labels.dat', 'w') as truth_out:\n",
    "                input_out.write('\\n'.join([' '.join(x.flatten()) for x in test_input.astype(str)]))\n",
    "                truth_out.write('\\n'.join(test_truth.astype(str)))\n",
    "        else:\n",
    "            test_data_prefix = '#'\n",
    "            \n",
    "        # Parse formatted cfg\n",
    "        cfg = yaml.safe_load(cfg_template.format(name=model.name, io_type=io_type, ap_type=ap_type, \n",
    "                                                 strategy=strategy, test_data_prefix=test_data_prefix))\n",
    "                \n",
    "        # Convert model\n",
    "        hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "        hls_model.compile()\n",
    "        \n",
    "        if show_quantization_plots:\n",
    "            ap, wp = hls4ml.model.profiling.numerical(keras_model=model, hls_model=hls_model, X=test_input)\n",
    "            plt.show()\n",
    "    \n",
    "        # Synthesize Model\n",
    "        hls_model.build(csim=csim, synth=True, cosim=cosim)\n",
    "        \n",
    "        if validate_sim_results:\n",
    "            if csim:\n",
    "                csim_binary_accuracy, csim_auc = calculate_metrics(test_truth, f'{model.name}/tb_data/csim_results.log')\n",
    "                print('-'*50)\n",
    "                print(model.name + ' CSIM Metrics')\n",
    "                print(f'binary accuracy = {csim_binary_accuracy}')\n",
    "                print(f'AUC = {csim_auc}')\n",
    "                print('-'*50)\n",
    "            if cosim:\n",
    "                cosim_binary_accuracy, cosim_auc = calculate_metrics(test_truth, f'{model.name}/tb_data/rtl_cosim_results.log')\n",
    "                print('-'*50)\n",
    "                print(model.name + ' CO-SIM Metrics')\n",
    "                print(f'binary accuracy = {cosim_binary_accuracy}')\n",
    "                print(f'AUC = {cosim_auc}')\n",
    "                print('-'*50)\n",
    "            \n",
    "        # grab xml data for synthesis (and possbily cosim)\n",
    "        with open(f'{model.name}/{model.name}_prj/solution1/syn/report/csynth.xml', 'rb') as fin:\n",
    "            syn_xml = xmltodict.parse(fin)\n",
    "        \n",
    "        if cosim:\n",
    "            sim_rpt = parse_sim_rpt(f'{model.name}/{model.name}_prj/solution1/sim/report/verilog/lat.rpt')\n",
    "                \n",
    "        \n",
    "        # Move back into previous wd\n",
    "        os.chdir('../..')\n",
    "        \n",
    "        if delete_files:\n",
    "            shutil.rmtree(path)\n",
    "        \n",
    "    except:\n",
    "        os.chdir('../..')\n",
    "        raise\n",
    "    \n",
    "    return {\n",
    "        'output_metrics': {\n",
    "            'csim': {\n",
    "                'binary_accuracy': csim_binary_accuracy,\n",
    "                'auc': csim_auc\n",
    "            },\n",
    "            'cosim': {\n",
    "                'binary_accuracy': cosim_binary_accuracy,\n",
    "                'auc': cosim_auc\n",
    "            }\n",
    "        },\n",
    "        'reports': {\n",
    "            'syn_xml': syn_xml,\n",
    "            'sim_rpt': sim_rpt\n",
    "        },\n",
    "        'syn_latency_estimates': {\n",
    "            'best': syn_xml['profile']['PerformanceEstimates']['SummaryOfOverallLatency']['Best-caseLatency'],\n",
    "            'avg': syn_xml['profile']['PerformanceEstimates']['SummaryOfOverallLatency']['Average-caseLatency'],\n",
    "            'worst': syn_xml['profile']['PerformanceEstimates']['SummaryOfOverallLatency']['Worst-caseLatency']\n",
    "        }\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
